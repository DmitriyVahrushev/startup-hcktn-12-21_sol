{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef8c9f1-70f4-4e70-82c1-28b999faa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import mailbox\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699765cf-79c2-4232-8676-d51f3de25d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(432)\n",
    "np.random.seed(432)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468126e-c5ec-49d5-b3f6-8b5675b7b53b",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15313fe1-8905-437a-9877-01c737ba8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcharsets(msg):\n",
    "    charsets = set({})\n",
    "    for c in msg.get_charsets():\n",
    "        if c is not None:\n",
    "            charsets.update([c])\n",
    "    return charsets\n",
    "\n",
    "def handleerror(errmsg, emailmsg,cs):\n",
    "    print()\n",
    "    print(errmsg)\n",
    "    print(\"This error occurred while decoding with \",cs,\" charset.\")\n",
    "    print(\"These charsets were found in the one email.\",getcharsets(emailmsg))\n",
    "    print(\"This is the subject:\",emailmsg['subject'])\n",
    "    print(\"This is the sender:\",emailmsg['From'])\n",
    "    \n",
    "def getbodyfromemail(msg):\n",
    "    body = None\n",
    "    #Walk through the parts of the email to find the text body.    \n",
    "    if msg.is_multipart():    \n",
    "        for part in msg.walk():\n",
    "\n",
    "            # If part is multipart, walk through the subparts.            \n",
    "            if part.is_multipart(): \n",
    "\n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        # Get the subpart payload (i.e the message body)\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #charset = subpart.get_charset()\n",
    "                    elif subpart.get_content_type() == 'text/html':\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #body = BeautifulSoup(body, \"lxml\").text\n",
    "\n",
    "            # Part isn't multipart so get the email body\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "                #charset = part.get_charset()\n",
    "            elif part.get_content_type() == 'text/html':\n",
    "                body = part.get_payload(decode=True) \n",
    "                #body = BeautifulSoup(body, \"lxml\").text\n",
    "\n",
    "    # If this isn't a multi-part message then get the payload (i.e the message body)\n",
    "    elif msg.get_content_type() == 'text/plain':\n",
    "        body = msg.get_payload(decode=True) \n",
    "    elif msg.get_content_type() == 'text/html':\n",
    "        body = msg.get_payload(decode=True) \n",
    "        #body = BeautifulSoup(body, \"lxml\").text\n",
    "    \n",
    "\n",
    "   # No checking done to match the charset with the correct part. \n",
    "#     for charset in getcharsets(msg):\n",
    "#         try:\n",
    "#             body = body.decode(charset)\n",
    "#         except UnicodeDecodeError:\n",
    "#             handleerror(\"UnicodeDecodeError: encountered.\",msg,charset)\n",
    "#         except AttributeError:\n",
    "#              handleerror(\"AttributeError: encountered\" ,msg,charset)\n",
    "    return body    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e777c8-cfa1-4f54-aeca-a38fdc906069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_emails(mbox_messages):\n",
    "    email_bodies = []\n",
    "    email_subjects = []\n",
    "    email_ids = []\n",
    "    email_content_types = []\n",
    "    for i, message in enumerate(mbox_messages):\n",
    "        body = getbodyfromemail(message)\n",
    "        email_bodies.append(body)\n",
    "#         body = message.get_payload()\n",
    "#         if isinstance(body, str):\n",
    "#             email_bodies.append(body)\n",
    "#         else:\n",
    "#             new_body = ' '.join(body)\n",
    "#             email_bodies.append(new_body)\n",
    "        if message['Subject']:\n",
    "            email_subjects.append(message['Subject'])\n",
    "        else:\n",
    "            email_subjects.append(\"Empty\")\n",
    "        \n",
    "        email_ids.append(message['X-UID'])\n",
    "        if message['Content-Type']:\n",
    "            email_content_types.append(message['Content-Type'])\n",
    "        else:\n",
    "            email_content_types.append(\"Empty\")\n",
    "    return email_bodies, email_subjects, email_ids, email_content_types\n",
    "\n",
    "\n",
    "def del_punct_symbols(texts):\n",
    "    texts = [str(text).lower().replace('\\n',' ').replace('\\t',' ') for text in texts]\n",
    "    texts = [re.sub(r'<.*?>','',str(text)) for text in texts]\n",
    "    texts = [re.sub(r'0x*?','',str(text)) for text in texts]\n",
    "    texts = [re.sub(r'[^\\w\\s]','',str(text)) for text in texts]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def del_stop_words(texts, stop_words):\n",
    "    return [[word for word in email.split() if word not in stop_words] for email in texts]\n",
    "\n",
    "\n",
    "def lemmatize_text(texts, lemmatizer):\n",
    "    return [[lemmatizer.lemmatize(word) for word in email] for email in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e379e9-14f1-4f15-86f2-45296eb85082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3322bd94-7daf-4b81-bb2b-4e8f64c92e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages = mailbox.mbox('train.mbox')\n",
    "train_bodies, train_subjects, train_ids, train_content_types = parse_emails(train_messages)\n",
    "train_bodies = lemmatize_text(del_stop_words(del_punct_symbols(train_bodies), stop_words), lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4051e0c9-d73f-4b5c-b937-66fd95aef872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fdsf ds x215443'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = 'fdsf ds 0x215443'\n",
    "re.sub(r'0x*?',r'', s) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0e981-b04e-48f2-99e5-4ea3e926fdf9",
   "metadata": {},
   "source": [
    "### DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d641176-d202-4f23-8980-7144454084b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "#train_data_bds, test_data_bds = train_bodies[:3000], train_bodies[3000:] \n",
    "train_data_bds = train_bodies\n",
    "train_data_bds_tagged = list(create_tagged_document(train_data_bds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ffd08d6-c9d4-4b89-9ea4-164c47184683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_data_bds_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406a4ca0-8537-49d3-8178-3bc36f6ebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_data_bds_tagged, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e0955d-efaf-4323-b859-9b4128e7276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4228, 1: 46, 3: 16, 2: 9, 7: 4, 9: 3, 6: 3, 4: 3, 44: 2, 15: 2, 11: 2, 18: 2, 2969: 1, 1134: 1, 23: 1, 17: 1, 60: 1, 587: 1, 876: 1, 2700: 1, 1659: 1, 399: 1, 3005: 1, 251: 1, 1203: 1, 4143: 1, 1695: 1, 641: 1, 70: 1, 4307: 1, 2622: 1, 1757: 1, 128: 1, 1302: 1, 3627: 1, 749: 1, 63: 1, 3534: 1, 2886: 1, 3681: 1, 26: 1, 5: 1, 76: 1, 1565: 1, 13: 1, 205: 1, 37: 1, 2858: 1, 1733: 1, 121: 1, 141: 1, 555: 1, 4145: 1, 4069: 1, 1845: 1, 10: 1, 247: 1, 697: 1, 35: 1, 8: 1, 2304: 1, 374: 1, 617: 1, 3839: 1, 4231: 1, 1549: 1, 4093: 1, 2734: 1, 50: 1, 31: 1, 89: 1, 3313: 1, 16: 1, 3120: 1, 1420: 1, 24: 1, 990: 1, 3615: 1, 3668: 1, 4257: 1, 2400: 1, 3532: 1, 3898: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_data_bds_tagged)):\n",
    "    inferred_vector = model.infer_vector(train_data_bds_tagged[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feee4876-d039-41f9-a42c-35caf562cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([model.infer_vector(vec) for vec in train_data_bds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8003e-3088-41b7-b4f5-359a9797fc6d",
   "metadata": {},
   "source": [
    "### OneClassSVM, LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d98e796c-8793-4795-8b9e-14c1b177f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf = OneClassSVM(gamma='auto').fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b00cad-fefc-4a44-8e69-cc4a5135f2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocalOutlierFactor(contamination=0.5, novelty=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "model_lof = LocalOutlierFactor(n_neighbors=20,leaf_size=30, contamination = 0.5, novelty=True)\n",
    "model_lof.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b364676-4306-435f-9905-5b268e100b9d",
   "metadata": {},
   "source": [
    "### Binary classifier with external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a61101e7-c8f6-4d0e-8ec7-406c0111f5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4391"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phish_bodies[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf844fc0-8ba1-40de-8427-5e450c560bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3088fc79-7de5-466b-8f9a-b0fbe7fd2a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051bd53-e2c3-4a4d-a64e-a212f6b68306",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "201a5017-118b-41fe-a800-6333623058bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 00:09:06,187 - gensim.utils - INFO - Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec_model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-12-12T00:09:06.187873', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 19:58:26) \\n[GCC 7.3.0]', 'platform': 'Linux-5.4.0-91-generic-x86_64-with-glibc2.10', 'event': 'saving'}\n",
      "2021-12-12 00:09:06,190 - gensim.utils - INFO - not storing attribute cum_table\n",
      "2021-12-12 00:09:06,235 - gensim.utils - INFO - saved doc2vec_model\n"
     ]
    }
   ],
   "source": [
    "model.save('doc2vec_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9ef94f-0e86-4d57-b268-29666212adea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lof_model.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "svm_filename = 'oneclass_svm_model.sav'\n",
    "joblib.dump(clf, svm_filename)\n",
    "lof_filename = 'lof_model.sav'\n",
    "joblib.dump(model_lof, lof_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb71e2-a26a-49ac-bb7c-e195355dce88",
   "metadata": {},
   "source": [
    "### Test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "491bf5e4-82b6-4019-8817-eda8e4e265aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = mailbox.mbox('test.mbox')\n",
    "test_bodies, test_subjects, test_ids, test_content_types = parse_emails(test_messages)\n",
    "test_bodies = lemmatize_text(del_stop_words(del_punct_symbols(test_bodies), stop_words), lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3662f7f-3a1a-450f-8b3d-268bc895dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = mailbox.mbox('test.mbox')\n",
    "test_bodies, test_subjects, test_ids, test_content_types = parse_emails(test_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6397444c-ea20-4f98-bcd8-b301dfb9a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bodies = [[word for word in text if word[0]!='x'] for text in test_bodies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "78a766f2-af43-4de5-8474-120d1766db37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bnnnnndear',\n",
       " 'business',\n",
       " 'client',\n",
       " 'region',\n",
       " 'banknthe',\n",
       " 'region',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'request',\n",
       " 'complete',\n",
       " 'region',\n",
       " 'interact',\n",
       " 'confirmation',\n",
       " 'formnthis',\n",
       " 'procedure',\n",
       " 'obligatory',\n",
       " 'business',\n",
       " 'corporate',\n",
       " 'client',\n",
       " 'region',\n",
       " 'banknplease',\n",
       " 'select',\n",
       " 'hyperlink',\n",
       " 'visit',\n",
       " 'address',\n",
       " 'listed',\n",
       " 'access',\n",
       " 'region',\n",
       " 'interact',\n",
       " 'confirmation',\n",
       " 'formnhttpinteractsession3627896regionscomibsregionscmserveriformcfmnagain',\n",
       " 'thank',\n",
       " 'choosing',\n",
       " 'region',\n",
       " 'bank',\n",
       " 'business',\n",
       " 'need',\n",
       " 'look',\n",
       " 'forward',\n",
       " 'working',\n",
       " 'youn',\n",
       " 'please',\n",
       " 'respond',\n",
       " 'email',\n",
       " 'mail',\n",
       " 'generated',\n",
       " 'automated',\n",
       " 'servicenreplies',\n",
       " 'mail',\n",
       " 'read',\n",
       " 'region',\n",
       " 'bank',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'technical',\n",
       " 'supportnncvs',\n",
       " 'start',\n",
       " 'exe',\n",
       " 'qtq7',\n",
       " 'hgnw',\n",
       " 'tmp',\n",
       " 'p9r',\n",
       " 'z433',\n",
       " 'engine',\n",
       " 'update',\n",
       " 'api',\n",
       " 'revision',\n",
       " 'juh',\n",
       " 'o2q',\n",
       " 'e9n',\n",
       " 'gbv',\n",
       " 'hex',\n",
       " 'serv',\n",
       " 'rcs',\n",
       " '7eob',\n",
       " 'revision',\n",
       " 'revision',\n",
       " '6233494',\n",
       " 'nmx4',\n",
       " 'hex',\n",
       " 'hex',\n",
       " 'file',\n",
       " 'dvk',\n",
       " 'fyd',\n",
       " 'tmp',\n",
       " 'serv',\n",
       " '5z8',\n",
       " 'interface',\n",
       " 'qstnrcs',\n",
       " 'rj7z',\n",
       " 'interface',\n",
       " '8lmo',\n",
       " 'cns',\n",
       " 'uyr',\n",
       " '1xl',\n",
       " 'file',\n",
       " 'qva',\n",
       " '99xa',\n",
       " 'uwi',\n",
       " 'k8ek']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bodies[13][:-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5bbfa571-7d50-4b9a-a275-17605ce0d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([model.infer_vector(vec) for vec in test_bodies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4f74aab7-d152-44a2-b4a2-188b3aabe6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict(X_test)\n",
    "res[res==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "50629890-7d29-4745-aa20-2769fc180f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_lof = model_lof.predict(X_test)\n",
    "res_lof[res_lof==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "06857e35-ff87-4be4-b817-d3bb6949e8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 8648]\n",
      " [   1 2011]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(res, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "747872c7-b7eb-432d-805a-228d4159c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 9383]\n",
      " [   1 1276]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(res_lof, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddc98d-6edc-486f-8459-4839b849ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bodies[9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c353c629-0c3a-45fa-a00e-4ff390da77a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multipart/related; \\n\\tboundary=\"D4RXVN07GRQFT4S3W\"'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content_types[5002]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065ac0f-eb42-47d2-b8eb-262ffe1474ea",
   "metadata": {},
   "source": [
    "## Accuracy estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54cb7d86-a168-462f-aad7-8e14a85405b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0out of 10659\n",
      "1000out of 10659\n",
      "2000out of 10659\n",
      "3000out of 10659\n",
      "4000out of 10659\n",
      "5000out of 10659\n",
      "6000out of 10659\n",
      "7000out of 10659\n",
      "8000out of 10659\n",
      "9000out of 10659\n",
      "10000out of 10659\n"
     ]
    }
   ],
   "source": [
    "targets = []\n",
    "test_ids_2 = []\n",
    "for i, test_message in enumerate(test_bodies):\n",
    "    is_real = 1\n",
    "    try:\n",
    "        if 'html' in test_content_types[i] or 'bound' in test_content_types[i]:\n",
    "            is_real = 0\n",
    "    except:\n",
    "        is_real = 0\n",
    "    targets.append(is_real)\n",
    "    if i%1000==0:\n",
    "        print(str(i)+\"out of \"+str(len(test_messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65ca4a3f-63d7-4313-a83f-5800215f088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_data = zip(test_ids, np.array(targets))\n",
    "# target_data_df = pd.DataFrame(target_data, columns = ['UID', 'VERDICT'])\n",
    "# target_data_df.to_csv('target.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5aa44f32-8f50-45b6-9dce-11262ee55f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 6064]\n",
      " [   1 4595]]\n"
     ]
    }
   ],
   "source": [
    "targets = np.array(targets)\n",
    "unique, counts = np.unique(targets, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f1ddf291-e026-4a34-b363-aafed8c269ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6725771648372267"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(targets, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "53099e0f-282d-4de6-9319-9f356d0aa2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6347687400318979"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(targets, res_lof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85e1e0-e790-4c29-b70e-372fbf1608b3",
   "metadata": {},
   "source": [
    "### Making submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7775b678-73a5-40db-9d90-4b10bc0c2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = zip(test_ids, res)\n",
    "submission_df = pd.DataFrame(submission_data, columns = ['UID', 'VERDICT'])\n",
    "submission_df.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e8079fa8-cfe3-4573-961c-dcad4de4316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = zip(test_ids, res_lof)\n",
    "submission_df = pd.DataFrame(submission_data, columns = ['UID', 'VERDICT'])\n",
    "submission_df.to_csv('result_lof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34febd4b-623e-49c8-8a84-a415fb186d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
