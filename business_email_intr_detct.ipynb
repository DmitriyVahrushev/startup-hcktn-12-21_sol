{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ef8c9f1-70f4-4e70-82c1-28b999faa6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import mailbox\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "699765cf-79c2-4232-8676-d51f3de25d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(432)\n",
    "np.random.seed(432)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468126e-c5ec-49d5-b3f6-8b5675b7b53b",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "15313fe1-8905-437a-9877-01c737ba8176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcharsets(msg):\n",
    "    charsets = set({})\n",
    "    for c in msg.get_charsets():\n",
    "        if c is not None:\n",
    "            charsets.update([c])\n",
    "    return charsets\n",
    "\n",
    "def handleerror(errmsg, emailmsg,cs):\n",
    "    print()\n",
    "    print(errmsg)\n",
    "    print(\"This error occurred while decoding with \",cs,\" charset.\")\n",
    "    print(\"These charsets were found in the one email.\",getcharsets(emailmsg))\n",
    "    print(\"This is the subject:\",emailmsg['subject'])\n",
    "    print(\"This is the sender:\",emailmsg['From'])\n",
    "    \n",
    "def getbodyfromemail(msg):\n",
    "    body = None\n",
    "    #Walk through the parts of the email to find the text body.    \n",
    "    if msg.is_multipart():    \n",
    "        for part in msg.walk():\n",
    "\n",
    "            # If part is multipart, walk through the subparts.            \n",
    "            if part.is_multipart(): \n",
    "\n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        # Get the subpart payload (i.e the message body)\n",
    "                        body = subpart.get_payload(decode=True) \n",
    "                        #charset = subpart.get_charset()\n",
    "\n",
    "            # Part isn't multipart so get the email body\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "                #charset = part.get_charset()\n",
    "\n",
    "    # If this isn't a multi-part message then get the payload (i.e the message body)\n",
    "    elif msg.get_content_type() == 'text/plain':\n",
    "        body = msg.get_payload(decode=True) \n",
    "\n",
    "   # No checking done to match the charset with the correct part. \n",
    "#     for charset in getcharsets(msg):\n",
    "#         try:\n",
    "#             body = body.decode(charset)\n",
    "#         except UnicodeDecodeError:\n",
    "#             handleerror(\"UnicodeDecodeError: encountered.\",msg,charset)\n",
    "#         except AttributeError:\n",
    "#              handleerror(\"AttributeError: encountered\" ,msg,charset)\n",
    "    return body    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0e777c8-cfa1-4f54-aeca-a38fdc906069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_emails(mbox_messages):\n",
    "    email_bodies = []\n",
    "    email_subjects = []\n",
    "    email_ids = []\n",
    "    email_content_types = []\n",
    "    for i, message in enumerate(mbox_messages):\n",
    "        body = getbodyfromemail(message)\n",
    "        email_bodies.append(body)\n",
    "#         body = message.get_payload()\n",
    "#         if isinstance(body, str):\n",
    "#             email_bodies.append(body)\n",
    "#         else:\n",
    "#             new_body = ' '.join(body)\n",
    "#             email_bodies.append(new_body)\n",
    "        if message['Subject']:\n",
    "            email_subjects.append(message['Subject'])\n",
    "        else:\n",
    "            email_subjects.append(\"Empty\")\n",
    "        email_ids.append(message['X-UID'])\n",
    "        if message['Content-Type']:\n",
    "            email_content_types.append(message['Content-Type'])\n",
    "        else:\n",
    "            email_content_types.append(\"Empty\")\n",
    "    return email_bodies, email_subjects, email_ids, email_content_types\n",
    "\n",
    "\n",
    "def del_punct_symbols(texts):\n",
    "    texts = [str(text).lower().replace('\\n',' ').replace('\\t',' ') for text in texts]\n",
    "    texts = [re.sub(r'[^\\w\\s]','',str(text)) for text in texts]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def del_stop_words(texts, stop_words):\n",
    "    return [[word for word in email.split() if word not in stop_words] for email in texts]\n",
    "\n",
    "\n",
    "def lemmatize_text(texts, lemmatizer):\n",
    "    return [[lemmatizer.lemmatize(word) for word in email] for email in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "79e379e9-14f1-4f15-86f2-45296eb85082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/dmitriy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3322bd94-7daf-4b81-bb2b-4e8f64c92e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_messages = mailbox.mbox('train.mbox')\n",
    "train_bodies, train_subjects, train_ids, train_content_types = parse_emails(train_messages)\n",
    "train_bodies = lemmatize_text(del_stop_words(del_punct_symbols(train_bodies), stop_words), lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f0e981-b04e-48f2-99e5-4ea3e926fdf9",
   "metadata": {},
   "source": [
    "### DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d641176-d202-4f23-8980-7144454084b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "train_data_bds, test_data_bds = train_bodies[:3000], train_bodies[3000:] \n",
    "train_data_bds_tagged = list(create_tagged_document(train_data_bds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ffd08d6-c9d4-4b89-9ea4-164c47184683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(train_data_bds_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "406a4ca0-8537-49d3-8178-3bc36f6ebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_data_bds_tagged, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "82e0955d-efaf-4323-b859-9b4128e7276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 2888, 1: 40, 2: 9, 3: 3, 9: 2, 21: 2, 12: 2, 5: 2, 8: 1, 6: 1, 772: 1, 2242: 1, 14: 1, 20: 1, 107: 1, 500: 1, 2999: 1, 488: 1, 58: 1, 71: 1, 2628: 1, 315: 1, 1283: 1, 2365: 1, 273: 1, 1768: 1, 2747: 1, 1148: 1, 164: 1, 2652: 1, 2013: 1, 1046: 1, 140: 1, 13: 1, 2614: 1, 1113: 1, 2966: 1, 54: 1, 286: 1, 1819: 1, 1198: 1, 1420: 1, 2294: 1, 4: 1, 1800: 1, 29: 1, 10: 1, 175: 1, 87: 1, 800: 1, 210: 1, 36: 1, 1708: 1, 1375: 1, 479: 1, 2939: 1, 2779: 1, 1186: 1, 552: 1, 123: 1})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_data_bds_tagged)):\n",
    "    inferred_vector = model.infer_vector(train_data_bds_tagged[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "feee4876-d039-41f9-a42c-35caf562cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([model.infer_vector(vec) for vec in test_data_bds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8003e-3088-41b7-b4f5-359a9797fc6d",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d98e796c-8793-4795-8b9e-14c1b177f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf = OneClassSVM(gamma='auto').fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb71e2-a26a-49ac-bb7c-e195355dce88",
   "metadata": {},
   "source": [
    "### Test data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "491bf5e4-82b6-4019-8817-eda8e4e265aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = mailbox.mbox('test.mbox')\n",
    "test_bodies, test_subjects, test_ids, test_content_types = parse_emails(test_messages)\n",
    "test_bodies = lemmatize_text(del_stop_words(del_punct_symbols(test_bodies), stop_words), lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bbfa571-7d50-4b9a-a275-17605ce0d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([model.infer_vector(vec) for vec in test_bodies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f74aab7-d152-44a2-b4a2-188b3aabe6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict(X_test)\n",
    "res[res==-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "06857e35-ff87-4be4-b817-d3bb6949e8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2726]\n",
      " [   1 7933]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(res, return_counts=True)\n",
    "\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85e1e0-e790-4c29-b70e-372fbf1608b3",
   "metadata": {},
   "source": [
    "### Making submission csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7775b678-73a5-40db-9d90-4b10bc0c2f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = zip(test_ids, res)\n",
    "submission_df = pd.DataFrame(submission_data, columns = ['UID', 'VERDICT'])\n",
    "submission_df.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613b7c0-e94e-4d88-93d7-a3a0a9cd4842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
